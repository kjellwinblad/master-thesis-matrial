\relax 
\citation{yamada2003statistical}
\citation{goldberg2008splitsvm}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:introduction}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Problem Description}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Problem Statement}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Goals}{2}}
\newlabel{sec:goals}{{1.1.2}{2}}
\citation{kubler2009dependency}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:background}{{2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Dependency Grammar}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The figure shows the dependency grammar structure for a sentence. E.g the node University has an edge labeled NAME to the word Uppsala, which describes a dependency relation between University and Uppsala.$^{1}$}}{5}}
\newlabel{fig:dependency_grammar_graph}{{2.1}{5}}
\citation{kubler2009dependency}
\citation{nivre2008algorithms}
\citation{kubler2009dependency}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Dependency Parsing}{6}}
\newlabel{sec:dependency_parsing}{{2.2}{6}}
\citation{kubler2009dependency}
\citation{nivre2008algorithms}
\citation{nivre2008algorithms}
\citation{boser1992training}
\citation{TanSK2005}
\citation{TanSK2005}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Measuring the Accuracy of Dependency Parsing Systems}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Support Vector Machines}{8}}
\newlabel{sec:svm}{{2.4}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}The Basic Support Vector Machine Concept}{8}}
\citation{keerthi2008sequential}
\citation{TanSK2005}
\citation{keerthi2008sequential}
\citation{safavian1991survey}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The maximum margin hyperplane that separates two classes in 2-dimensional space.}}{9}}
\newlabel{fig:maximum_margin_diagram}{{2.2}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}The Kernel Trick}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}The Extension to Support Multiple Class Classification}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Decision Trees}{9}}
\citation{TanSK2005}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Training examples for the decision tree example.}}{10}}
\newlabel{tab:trainingset}{{2.1}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The figure shows an example of a decision tree, where $s0(X)$ represent that the word on top of the stack has the word type $X$ and $b0(X)$ that the first word in the buffer has X as word type.}}{10}}
\newlabel{fig:decision_tree_exampl}{{2.3}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Gain Ratio}{10}}
\newlabel{sec:gain_ratio}{{2.5.1}{10}}
\citation{quinlan1993c4}
\citation{shannon1948mathematical}
\citation{TanSK2005}
\newlabel{eq:entropy}{{2.1}{11}}
\newlabel{eq:information_gain}{{2.2}{11}}
\newlabel{eq:split_info}{{2.3}{11}}
\newlabel{eq:gain_ratio}{{2.4}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Measuring the Accuracy of Machine Learning Methods with Cross-Validation}{12}}
\citation{goldberg2008splitsvm}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Hypotheses and Methodology}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:hypotheses_and_methodology}{{3}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Hypotheses}{13}}
\newlabel{sec:hypotheses}{{3.1}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Methods}{13}}
\newlabel{sec:methods}{{3.2}{13}}
\citation{chang2001libsvm}
\citation{fan2008liblinear}
\citation{nilsson2007conll}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Tools}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}MaltParser}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}The Support Vector Machine Libraries LIBSVM and LIBLINEAR}{14}}
\newlabel{sec:liblinear_libsvm}{{3.3.2}{14}}
\citation{Buchholz06conll-xshared}
\citation{Hajic09theconll-2009}
\citation{nivre2006talbanken05}
\citation{chen2003sinica}
\citation{brants2002tiger}
\citation{hajic2006prague}
\citation{Hajic09theconll-2009}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Data Sets}{15}}
\newlabel{sec:data_sets}{{3.4}{15}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Treebanks used in the experiments described in chapter\nobreakspace  {}4\hbox {}.}}{15}}
\newlabel{tab:trainingsets}{{3.1}{15}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments}{17}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:experiments}{{4}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Division of the Training Set With Linear and Nonlinear SVMs}{17}}
\newlabel{sec:experiment_division_of_training_set_with_liblinear_and_libsvm}{{4.1}{17}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces The table contains the results for the tree languages Swedish, Chinese and German tested with a linear SVM with and without division. \textbf  {TR} = \emph  {training time in hours}, \textbf  {TE} = \emph  {testing time in hours}, \textbf  {AC} = \emph  {Labeled Attachment Score}}}{18}}
\newlabel{tab:experiment_1_liblinear}{{4.1}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Results and Discussion}{18}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces The table contains the results for the tree languages Swedish, Chinese and German tested with a nonlinear SVM with and without division. The results for the \emph  {German} with the biggest training set is not included in the results because of too long calculation time. \textbf  {TR} = \emph  {training time in hours}, \textbf  {TE} = \emph  {testing time in hours}, \textbf  {AC} = \emph  {Labeled Attachment Score}}}{19}}
\newlabel{tab:experiment_1_libsvm}{{4.2}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Accuracy of Partitions Created by Division}{19}}
\newlabel{sec:experiment_accuracy_of_partitions_created_by_division}{{4.2}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Results and Discussion}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The diagram shows three median based box plots for the accuracy of the partitions created by division. Plot A is for Swedish, B is for Chinese and C is for German.}}{20}}
\newlabel{fig:accuracy_of_partions}{{4.1}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Another Division of the Worst Performing Partitions}{21}}
\newlabel{sec:an_other_division_of_the_worst_performing_partitions_after_the_first_division}{{4.3}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Results and Discussion}{21}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces The table presents the results of the partitions performing worse and better separately when using feature 1 to divide the training instances. The columns named \emph  {Size} in the table represent the fraction of the total number of instances that where in the worst performing partitions and in the best performing partitions. The columns named \emph  {Feat. 1} represents the test case when the POSTAG property of the first element in the buffer was used to divide the instances. The columns named \emph  {Feat. 2} represents the test case when the POSTAG property of the first element in the stack was used to divide the instances. The columns named \emph  {No div.} represents the test case when all partitions were concatenated to one chunk.}}{22}}
\newlabel{tab:experiment_divide_with_an_other_feature_after}{{4.3}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Different Levels of Division}{22}}
\newlabel{sec:different_levels_of_divisions}{{4.4}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Results and Discussion}{22}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces The table shows weighted cross validation scores for the different levels of division. The columns with the header ''Sign.'' shows the statistical significance of the difference between the divisions. For example the statistical certainty that one division gives better accuracy than no divisions for a Swedish data set of the size used in the experiment is greater than 70\%. In other words, an element in a ''Sign.'' column shows the statistical confidence that there is a difference in accuracy between the method used to get the value to the left of the element and the method used to get the value to the right of the element. The estimation of the statistical certainty is based on the assumption that the cross validation accuracy has equal or better certainty than a test with a single test set of the same size as the test sets used in the cross validation$^{1}$. }}{23}}
\newlabel{tab:different_levels_of_division}{{4.4}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Decision Tree With Intuitive Division Order}{23}}
\newlabel{sec:decision_tree_with_intuitive_division_order}{{4.5}{23}}
\newlabel{tab:division_feature_table}{{4.5}{24}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces The table lists the intuitive division order used in the decision tree creation algorithm. Input[n] represents the n:th element on the buffer and Stack[n] represents the n:th value on the stack in the dependency parsing algorithm. E.g. Input[0] represents the first element in the buffer. POSTAG is the property used for all division features.}}{24}}
\newlabel{tab:division_feature_table}{{4.5}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Results and Discussion}{24}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces The table contains the decision tree algorithm used in the experiments.}}{25}}
\newlabel{tab:decision_tree_algorithm}{{4.6}{25}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Decision Tree With Devision Order Decided by Gain Ratio}{25}}
\newlabel{sec:decision_tree_with_devision_order_decided_by_gain_ratio}{{4.6}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Results and Discussion}{25}}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces The accuracy for different languages calculated in the decision tree experiment. The column named Intuitive represents the tree division with the intuitive division order and the column named Gain Ratio represent the tree division with division order calculated by Gain Ratio. See section\nobreakspace  {}4.6\hbox {} for an explanation of the Gain Ratio column and the description of table\nobreakspace  {}4.4\hbox {} for a description of the ''Sign.'' columns.}}{26}}
\newlabel{tab:decion_tree_with_and_without_gain_ratio}{{4.7}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Decision Tree in Malt Parser}{26}}
\newlabel{sec:decision_tree_in_malt_parser}{{4.7}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Results and Discussion}{27}}
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces Summary of table\nobreakspace  {}4.9\hbox {} containing only the accuracy of the test accuracy after training with the biggest data sets. The ''Sign'' columns shows the statistical significance of the difference between the two tree methods and the simple division strategy. For example the statistical certainty that the simple division strategy gives better accuracy than the tree division strategy with intuitive division order for a Swedish data set of the size used in the experiment is greater than 19\%$^{1}$.}}{27}}
\newlabel{tab:summary_decision_tree_in_malt_parser}{{4.8}{27}}
\@writefile{lot}{\contentsline {table}{\numberline {4.9}{\ignorespaces The table contains the results for the decision tree algorithm with LIBLINEAR implemented in MaltParser. The decision tree creation algorithm is presented in section\nobreakspace  {}4.5\hbox {}. \textbf  {TR} = \emph  {training time in hours}, \textbf  {TE} = \emph  {testing time in hours}, \textbf  {AC} = \emph  {Labeled Attachment Score}}}{28}}
\newlabel{tab:decion_tree_in_malt_parser}{{4.9}{28}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}MaltParser Plugin}{29}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:maltparser_plugin}{{5}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Implementation}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Usage}{29}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusions}{31}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:conclusions}{{6}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Limitations}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Future work}{32}}
\newlabel{sec:conclution_future_work}{{6.2}{32}}
\bibstyle{alpha}
\bibdata{base}
\bibcite{brants2002tiger}{BDH{$^{+}$}02}
\bibcite{boser1992training}{BGV92}
\bibcite{Buchholz06conll-xshared}{BM06}
\bibcite{chang2001libsvm}{CL01}
\bibcite{chen2003sinica}{CLC{$^{+}$}03}
\bibcite{fan2008liblinear}{FCH{$^{+}$}08}
\bibcite{goldberg2008splitsvm}{GE08}
\bibcite{Hajic09theconll-2009}{HCJ{$^{+}$}09}
\bibcite{hajic2006prague}{HPH{$^{+}$}}
\bibcite{kubler2009dependency}{KMN09}
\bibcite{keerthi2008sequential}{KSC{$^{+}$}08}
\@writefile{toc}{\contentsline {chapter}{References}{33}}
\bibcite{nilsson2007conll}{NHK{$^{+}$}07}
\bibcite{nivre2008algorithms}{Niv08}
\bibcite{nivre2006talbanken05}{NNH06}
\bibcite{quinlan1993c4}{Qui93}
\bibcite{shannon1948mathematical}{Sha48}
\bibcite{safavian1991survey}{SL91}
\bibcite{TanSK2005}{TSK05}
\bibcite{yamada2003statistical}{YM03}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Experiment Diagrams}{35}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:diagrams}{{A}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces The diagram illustrates that there is no clear pattern between size of partitions created when dividing the training data and the cross validation accuracy. Every spike in the diagram represents a partition created when dividing Swedish. The partition size in the diagram is the fraction of the total training set size.}}{35}}
\newlabel{fig:accuracy_of_partions_in_relationship_to_size_swe}{{A.1}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces The diagram illustrates that there is no clear pattern between size of partitions created when dividing the training data and the cross validation accuracy. Every spike in the diagram represents a partition created when dividing Chinese. The partition size in the diagram is the fraction of the total training set size.}}{36}}
\newlabel{fig:accuracy_of_partions_in_relationship_to_size_chi}{{A.2}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces The diagram illustrates that there is no clear pattern between size of partitions created when dividing the training data and the cross validation accuracy. Every spike in the diagram represents a partition created when dividing German. The partition size in the diagram is the fraction of the total training set size.}}{37}}
\newlabel{fig:accuracy_of_partions_in_relationship_to_size_ger}{{A.3}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces The digram visualizes the Labeled Attachment Score for all training set sizes created from the Swedish data set and tested in the experiment presented in section\nobreakspace  {}4.1\hbox {}. The values used to create the diagram can be seen in table\nobreakspace  {}4.1\hbox {} and 4.2\hbox {}. Please, note that the x-axis in the diagram has logarithmic scale.}}{38}}
\newlabel{fig:libsvm_liblinear_div_not_div_experiment_diagrams_swe}{{A.4}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.5}{\ignorespaces The digram visualizes the Labeled Attachment Score for all training set sizes created from the Chinese data set and tested in the experiment presented in section\nobreakspace  {}4.1\hbox {}. The values used to create the diagram can be seen in table\nobreakspace  {}4.1\hbox {} and 4.2\hbox {}. Please, note that the x-axis in the diagram has logarithmic scale.}}{39}}
\newlabel{fig:libsvm_liblinear_div_not_div_experiment_diagrams_chi}{{A.5}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.6}{\ignorespaces The digram visualizes the Labeled Attachment Score for all training set sizes created from the German data set and tested in the experiment presented in section\nobreakspace  {}4.1\hbox {}. The values used to create the diagram can be seen in table\nobreakspace  {}4.1\hbox {} and 4.2\hbox {}. Please, note that the x-axis in the diagram has logarithmic scale.}}{40}}
\newlabel{fig:libsvm_liblinear_div_not_div_experiment_diagrams_ger}{{A.6}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.7}{\ignorespaces The digram visualizes the Labeled Attachment Score for all training set sizes created from the Swedish data set and tested in the experiment presented in section\nobreakspace  {}4.7\hbox {}. The values used to create the diagram can be seen in table\nobreakspace  {}4.9\hbox {}. Please, note that the x-axis in the diagram has logarithmic scale.}}{41}}
\newlabel{fig:decion_tree_malt_parser_swe}{{A.7}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.8}{\ignorespaces The digram visualizes the Labeled Attachment Score for all training set sizes created from the Chinese data set and tested in the experiment presented in section\nobreakspace  {}4.7\hbox {}. The values used to create the diagram can be seen in table\nobreakspace  {}4.9\hbox {}. Please, note that the x-axis in the diagram has logarithmic scale.}}{42}}
\newlabel{fig:decion_tree_malt_parser_chi}{{A.8}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.9}{\ignorespaces The digram visualizes the Labeled Attachment Score for all training set sizes created from the German data set and tested in the experiment presented in section\nobreakspace  {}4.7\hbox {}. The values used to create the diagram can be seen in table\nobreakspace  {}4.9\hbox {}. Please, note that the x-axis in the diagram has logarithmic scale.}}{43}}
\newlabel{fig:decion_tree_malt_parser_ger}{{A.9}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.10}{\ignorespaces The digram visualizes the Labeled Attachment Score for all training set sizes created from the Czech data set and tested in the experiment presented in section\nobreakspace  {}4.7\hbox {}. The values used to create the diagram can be seen in table\nobreakspace  {}4.9\hbox {}. Please, note that the x-axis in the diagram has logarithmic scale.}}{44}}
\newlabel{fig:decion_tree_malt_parser_cze}{{A.10}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.11}{\ignorespaces The digram visualizes the Labeled Attachment Score for all training set sizes created from the English data set and tested in the experiment presented in section\nobreakspace  {}4.7\hbox {}. The values used to create the diagram can be seen in table\nobreakspace  {}4.9\hbox {}. Please, note that the x-axis in the diagram has logarithmic scale.}}{45}}
\newlabel{fig:decion_tree_malt_parser_eng}{{A.11}{45}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}MaltParser Settings}{47}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:appendix_maltparser_settings}{{B}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Basic Configuration}{47}}
\newlabel{sec:appendix_LIBLINEAR_DIV}{{B.1}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Advanced Feature Extraction Models for Czech and English}{48}}
\newlabel{app:feature_extraction_models_for_czech_english}{{B.2}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.1}English Stack Projective}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.2}English Stack Lazy}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.3}Czech Stack Projective}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.4}Czech Stack Lazy}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}LIBLINAR and LIBSVM settings}{51}}
\newlabel{app:LIBLINAR_and_LIBSVM_settings}{{B.3}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {B.4}Configuration for Division and Decision Tree in Malt Parser}{52}}
\newlabel{app:malt_parser_configuration_last_experiment}{{B.4}{52}}
