\relax 
\citation{nivre2006maltparser}
\citation{yamada2003statistical}
\citation{goldberg2008splitsvm}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:introduction}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Problem Description}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Problem Statement}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Goals}{2}}
\newlabel{sec:goals}{{1.1.2}{2}}
\citation{kubler2009dependency}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:background}{{2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Dependency Grammar}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The figure shows the dependency grammar structure for a sentence. E.g the node University has a edge labeled PMOD NAME to the word Uppsala, which describe a dependency relation between University and Uppsala.$^{1}$}}{5}}
\newlabel{fig:dependency_grammar_graph}{{2.1}{5}}
\citation{nivre2008algorithms}
\citation{kubler2009dependency}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Dependency Parsing}{6}}
\newlabel{sec:dependency_parsing}{{2.2}{6}}
\citation{nivre2008algorithms}
\citation{nivre2008algorithms}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Measuring the Accuracy of Dependency Parsing Systems}{7}}
\citation{boser1992training}
\citation{TanSK2005}
\citation{TanSK2005}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Support Vector Machines}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}The Basic Support Vector Machine Concept}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The figure illustrate the maximum margin hyperplane that separate two classes in 2-dimensional space.}}{8}}
\newlabel{fig:maximum_margin_diagram}{{2.2}{8}}
\citation{keerthi2008sequential}
\citation{TanSK2005}
\citation{keerthi2008sequential}
\citation{safavian1991survey}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}The Kernel Trick}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}The Extension to Support Multiple Class Classification}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Decision Trees}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces The table displays training examples for the decision tree example.}}{9}}
\newlabel{tab:trainingset}{{2.1}{9}}
\citation{TanSK2005}
\citation{quinlan1993c4}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The figure shows an example of a decision tree where $s0(X)$ represent that the word on top of the stack has the word type $X$, and $b0(X)$ that the first word in the buffer has X as word type.}}{10}}
\newlabel{fig:decision_tree_exampl}{{2.3}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Gain Ratio}{10}}
\newlabel{sec:gain_ratio}{{2.4.1}{10}}
\citation{shannon1948mathematical}
\citation{TanSK2005}
\newlabel{eq:entropy}{{2.1}{11}}
\newlabel{eq:information_gain}{{2.2}{11}}
\newlabel{eq:split_info}{{2.3}{11}}
\newlabel{eq:gain_ratio}{{2.4}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Measuring the Accuracy of Machine Learning Methods with Cross-Validation}{11}}
\citation{goldberg2008splitsvm}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Hypotheses and Methodology}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:hypotheses_and_methodology}{{3}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Hypotheses}{13}}
\newlabel{sec:hypotheses}{{3.1}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Methods}{14}}
\newlabel{sec:methods}{{3.2}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Tools}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}MaltParser}{14}}
\citation{chang2001libsvm}
\citation{fan2008liblinear}
\citation{odersky2004overview}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}The Support Vector Machine Libraries LIBSVM and LIBLINEAR}{15}}
\newlabel{sec:liblinear_libsvm}{{3.3.2}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}The Programming Language Used for the Experiments: Scala}{15}}
\citation{nilsson2007conll}
\citation{Buchholz06conll-xshared}
\citation{Hajic09theconll-2009}
\citation{nivre2006talbanken05}
\citation{chen2003sinica}
\citation{brants2002tiger}
\citation{hajic2006prague}
\citation{Hajic09theconll-2009}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Data Sets}{16}}
\newlabel{sec:data_sets}{{3.4}{16}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces The table shows the treebanks used in the experiments described in chapter\nobreakspace  {}4\hbox {}.}}{16}}
\newlabel{tab:trainingsets}{{3.1}{16}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments}{17}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:experiments}{{4}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Division of the Training Set With LIBLINEAR and LIBSVM}{17}}
\newlabel{sec:experiment_division_of_training_set_with_liblinear_and_libsvm}{{4.1}{17}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces The table contains the results for the tree languages Swedish, Chinese and German tested with a linear SVM with and without division. \textbf  {TR} = \emph  {training time in hours}, \textbf  {TE} = \emph  {testing time in hours}, \textbf  {AC} = \emph  {Label Attachment Score}}}{18}}
\newlabel{tab:experiment_1_liblinear}{{4.1}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Results and Discussion}{18}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces The table contains the results for the tree languages Swedish, Chinese and German tested with a nonlinear SVM with and without division. The results for the \emph  {German} with the biggest training set is not included in the results because of too long calculation time. \textbf  {TR} = \emph  {training time in hours}, \textbf  {TE} = \emph  {testing time in hours}, \textbf  {AC} = \emph  {Label Attachment Score}}}{19}}
\newlabel{tab:experiment_1_libsvm}{{4.2}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Accuracy of Partitions Created by Division}{19}}
\newlabel{sec:experiment_accuracy_of_partitions_created_by_division}{{4.2}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Results and Discussion}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The diagram shows three median based box plots for the accuracy of the partitions created by division. Plot A is for Swedish, B is for Chinese and C is for German.}}{20}}
\newlabel{fig:accuracy_of_partions}{{4.1}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}An Other Division of the Worst Performing Partitions After the First Division}{21}}
\newlabel{sec:an_other_division_of_the_worst_performing_partitions_after_the_first_division}{{4.3}{21}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces The table presents the results of the partitions performing worse and better separately when using feature 1 to divide the training instances. The columns named \emph  {Size} in the table represent the fraction of the total number of instances that where in the worst performing partitions and in the best performing partitions. The columns named \emph  {Feat. 1} represents the test case when the POSTAG property of the first element in the buffer was used to divide the instances. The columns named \emph  {Feat. 2} represents the test case when the POSTAG property of the first element in the stack was used to divide the instances. The columns named \emph  {No div.} represents the test case when all partitions were concatenated to one chunk.}}{21}}
\newlabel{tab:experiment_divide_with_an_other_feature_after}{{4.3}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Results and Discussion}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Different Levels of Division}{22}}
\newlabel{sec:different_levels_of_divisions}{{4.4}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Results and Discussion}{22}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces The table shows weighted cross validation scores for different levels of division. }}{22}}
\newlabel{tab:different_levels_of_division}{{4.4}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Decision Tree With Intuitive Division Order}{23}}
\newlabel{sec:decision_tree_with_intuitive_division_order}{{4.5}{23}}
\newlabel{tab:division_feature_table}{{4.5}{23}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces The table lists the intuitive division order used in the decision tree creation algorithm. Input[N] represents the n:th element on the buffer and Stack[n] represents the n:th value on the stack in the dependency parsing algorithm. E.g. Input[0] represents the first element in the buffer. POSTAG is the property used for all division features.}}{23}}
\newlabel{tab:division_feature_table}{{4.5}{23}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces The table contains the decision tree algorithm used in the experiments.}}{24}}
\newlabel{tab:decision_tree_algorithm}{{4.6}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Results and Discussion}{24}}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces The accuracy for different language calculated in the decision tree experiment. The column named Intuitive represents the tree division with the intuitive division order and the column named Gain Ratio represent the tree division with division order calculated by Gain Ratio. Please, see section\nobreakspace  {}4.6\hbox {} for an explanation of the Gain Ratio column.}}{25}}
\newlabel{tab:decion_tree_with_and_without_gain_ratio}{{4.7}{25}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Decision Tree With Devision Order Decided by Gain Ratio}{25}}
\newlabel{sec:decision_tree_with_devision_order_decided_by_gain_ratio}{{4.6}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Results and Discussion}{25}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Decision Tree in Malt Parser}{26}}
\newlabel{sec:decision_tree_in_malt_parser}{{4.7}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Results and Discussion}{26}}
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces The table contains the results for the decision tree algorithm with LIBLINEAR implemented in MaltParser. The decision tree creation algorithm is presented in section\nobreakspace  {}4.5\hbox {}. \textbf  {TR} = \emph  {training time in hours}, \textbf  {TE} = \emph  {testing time in hours}, \textbf  {AC} = \emph  {Label Attachment Score}}}{28}}
\newlabel{tab:decion_tree_in_malt_parser}{{4.8}{28}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}MaltParser Plugin}{29}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:maltparser_plugin}{{5}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Implementation}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Usage}{29}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusions}{31}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:conclusions}{{6}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Limitations}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Future work}{32}}
\bibstyle{alpha}
\bibdata{base}
\bibcite{brants2002tiger}{BDH{$^{+}$}02}
\bibcite{boser1992training}{BGV92}
\bibcite{Buchholz06conll-xshared}{BM06}
\bibcite{chang2001libsvm}{CL01}
\bibcite{chen2003sinica}{CLC{$^{+}$}03}
\bibcite{fan2008liblinear}{FCH{$^{+}$}08}
\bibcite{goldberg2008splitsvm}{GE08}
\bibcite{Hajic09theconll-2009}{HCJ{$^{+}$}09}
\bibcite{hajic2006prague}{HPH{$^{+}$}06}
\bibcite{kubler2009dependency}{KMN09}
\bibcite{keerthi2008sequential}{KSC{$^{+}$}08}
\bibcite{nivre2006maltparser}{NHN06}
\@writefile{toc}{\contentsline {chapter}{References}{33}}
\bibcite{nivre2008algorithms}{Niv08}
\bibcite{nivre2006talbanken05}{NNH06}
\bibcite{nilsson2007conll}{NRY07}
\bibcite{odersky2004overview}{OAC{$^{+}$}04}
\bibcite{quinlan1993c4}{Qui93}
\bibcite{shannon1948mathematical}{Sha48}
\bibcite{safavian1991survey}{SL91}
\bibcite{TanSK2005}{TSK05}
\bibcite{yamada2003statistical}{YM03}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Experiment Diagrams}{35}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:diagrams}{{A}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces The diagram illustrate that there is no clear pattern between size of partitions created when dividing the training data and the cross validation accuracy. Every spike in the diagram represents a partition created when dividing Swedish. The partition size in the diagram is the fraction of the total training set size.}}{35}}
\newlabel{fig:accuracy_of_partions_in_relationship_to_size_swe}{{A.1}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces The diagram illustrate that there is no clear pattern between size of partitions created when dividing the training data and the cross validation accuracy. Every spike in the diagram represents a partition created when dividing Chinese. The partition size in the diagram is the fraction of the total training set size.}}{36}}
\newlabel{fig:accuracy_of_partions_in_relationship_to_size_chi}{{A.2}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces The diagram illustrate that there is no clear pattern between size of partitions created when dividing the training data and the cross validation accuracy. Every spike in the diagram represents a partition created when dividing German. The partition size in the diagram is the fraction of the total training set size.}}{37}}
\newlabel{fig:accuracy_of_partions_in_relationship_to_size_ger}{{A.3}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces The digram visualize the Label Attachment Score for all training set sizes created from the Swedish data set and tested in the experiment presented in section\nobreakspace  {}4.1\hbox {}. The values used to create the diagram can be seen in table\nobreakspace  {}4.1\hbox {} and 4.2\hbox {}. Please, note that the x-axis in the diagram has logarithmic scale.}}{38}}
\newlabel{fig:libsvm_liblinear_div_not_div_experiment_diagrams_swe}{{A.4}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.5}{\ignorespaces The digram visualize the Label Attachment Score for all training set sizes created from the Chinese data set and tested in the experiment presented in section\nobreakspace  {}4.1\hbox {}. The values used to create the diagram can be seen in table\nobreakspace  {}4.1\hbox {} and 4.2\hbox {}. Please, note that the x-axis in the diagram has logarithmic scale.}}{39}}
\newlabel{fig:libsvm_liblinear_div_not_div_experiment_diagrams_chi}{{A.5}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.6}{\ignorespaces The digram visualize the Label Attachment Score for all training set sizes created from the German data set and tested in the experiment presented in section\nobreakspace  {}4.1\hbox {}. The values used to create the diagram can be seen in table\nobreakspace  {}4.1\hbox {} and 4.2\hbox {}. Please, note that the x-axis in the diagram has logarithmic scale.}}{40}}
\newlabel{fig:libsvm_liblinear_div_not_div_experiment_diagrams_ger}{{A.6}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.7}{\ignorespaces The digram visualize the Label Attachment Score for all training set sizes created from the Swedish data set and tested in the experiment presented in section\nobreakspace  {}4.7\hbox {}. The values used to create the diagram can be seen in table\nobreakspace  {}4.8\hbox {}. Please, note that the x-axis in the diagram has logarithmic scale.}}{41}}
\newlabel{fig:decion_tree_malt_parser_swe}{{A.7}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.8}{\ignorespaces The digram visualize the Label Attachment Score for all training set sizes created from the Chinese data set and tested in the experiment presented in section\nobreakspace  {}4.7\hbox {}. The values used to create the diagram can be seen in table\nobreakspace  {}4.8\hbox {}. Please, note that the x-axis in the diagram has logarithmic scale.}}{42}}
\newlabel{fig:decion_tree_malt_parser_chi}{{A.8}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.9}{\ignorespaces The digram visualize the Label Attachment Score for all training set sizes created from the German data set and tested in the experiment presented in section\nobreakspace  {}4.7\hbox {}. The values used to create the diagram can be seen in table\nobreakspace  {}4.8\hbox {}. Please, note that the x-axis in the diagram has logarithmic scale.}}{43}}
\newlabel{fig:decion_tree_malt_parser_ger}{{A.9}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.10}{\ignorespaces The digram visualize the Label Attachment Score for all training set sizes created from the Czech data set and tested in the experiment presented in section\nobreakspace  {}4.7\hbox {}. The values used to create the diagram can be seen in table\nobreakspace  {}4.8\hbox {}. Please, note that the x-axis in the diagram has logarithmic scale.}}{44}}
\newlabel{fig:decion_tree_malt_parser_cze}{{A.10}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.11}{\ignorespaces The digram visualize the Label Attachment Score for all training set sizes created from the English data set and tested in the experiment presented in section\nobreakspace  {}4.7\hbox {}. The values used to create the diagram can be seen in table\nobreakspace  {}4.8\hbox {}. Please, note that the x-axis in the diagram has logarithmic scale.}}{45}}
\newlabel{fig:decion_tree_malt_parser_eng}{{A.11}{45}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}MaltParser Settings}{47}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:appendix_maltparser_settings}{{B}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Basic Configuration}{47}}
\newlabel{sec:appendix_LIBLINEAR_DIV}{{B.1}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Advanced Feature Extraction Models for Czech and English}{48}}
\newlabel{app:feature_extraction_models_for_czech_english}{{B.2}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.1}English Stack Projection}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.2}English Stack Lazy}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.3}Czech Stack Projection}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.4}Czech Stack Lazy}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Configuration for Division and Decision Tree in Malt Parser}{52}}
\newlabel{app:malt_parser_configuration_last_experiment}{{B.3}{52}}
